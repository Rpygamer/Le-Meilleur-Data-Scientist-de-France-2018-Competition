{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-07T12:56:13.421259Z",
     "start_time": "2018-06-07T12:56:12.384977Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Initially forked from Bojan's kernel here: https://www.kaggle.com/tunguz/bow-meta-text-and-dense-features-lb-0-2242/code\n",
    "#That kernel was forked from Nick Brook's kernel here: https://www.kaggle.com/nicapotato/bow-meta-text-and-dense-features-lgbm?scriptVersionId=3493400\n",
    "#Used oof method from Faron's kernel here: https://www.kaggle.com/mmueller/stacking-starter?scriptVersionId=390867\n",
    "#Used some text cleaning method from Muhammad Alfiansyah's kernel here: https://www.kaggle.com/muhammadalfiansyah/push-the-lgbm-v19\n",
    "import time\n",
    "notebookstart= time.time()\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import gc\n",
    "\n",
    "\n",
    "# Models Packages\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import feature_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Gradient Boosting\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "# Tf-Idf\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "# Viz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "\n",
    "%matplotlib inline\n",
    "NFOLDS = 5\n",
    "SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-07T12:56:13.430589Z",
     "start_time": "2018-06-07T12:56:13.423166Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', colsample_bytree=1, learning_rate=0.1,\n",
       "       max_bin=255, max_depth=-1, min_child_samples=10, min_child_weight=5,\n",
       "       min_split_gain=0, n_estimators=10, nthread=-1, num_leaves=31,\n",
       "       objective='regression', reg_alpha=0, reg_lambda=0, seed=0,\n",
       "       silent=True, subsample=1, subsample_for_bin=50000, subsample_freq=1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb.LGBMRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-07T12:56:50.578191Z",
     "start_time": "2018-06-07T12:56:13.432716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Load Stage\n",
      "Train shape: 1503424 Rows, 16 Columns\n",
      "Test shape: 508438 Rows, 16 Columns\n",
      "Combine Train and Test\n",
      "\n",
      "All Data shape: 2011862 Rows, 16 Columns\n"
     ]
    }
   ],
   "source": [
    "class SklearnWrapper(object):\n",
    "    def __init__(self, clf, seed=0, params=None, seed_bool = True,lgbm=False):\n",
    "        if(seed_bool == True):\n",
    "            if lgbm:\n",
    "                print('with no random state need to check that')\n",
    "            else : \n",
    "                params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "        \n",
    "def get_oof(clf, x_train, y, x_test):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        print('\\nFold {}'.format(i))\n",
    "        x_tr = x_train.tocsr()[train_index]\n",
    "        y_tr = y[train_index]\n",
    "        x_te = x_train.tocsr()[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n",
    "    \n",
    "'''def cleanName(text):\n",
    "    try:\n",
    "        textProc = text.lower()\n",
    "        textProc = \" \".join(map(str.strip, re.split('(\\d+)',textProc)))\n",
    "        regex = re.compile(u'[^[:alpha:]]')\n",
    "        textProc = regex.sub(\" \", textProc)\n",
    "        textProc = \" \".join(textProc.split())\n",
    "        return textProc\n",
    "    except: \n",
    "        return \"name error\"'''\n",
    "\n",
    "def cleanName(text):\n",
    "    try:\n",
    "        textProc = text.lower()\n",
    "        textProc = \" \".join(map(str.strip, re.split('(\\d+)',textProc)))\n",
    "        regex = re.compile(u'[^[:alpha:]]')\n",
    "        textProc = regex.sub(\" \", textProc)\n",
    "        textProc = re.sub('[!@#$_“”¨«»®´·º½¾¿¡§£₤‘’]', '', textProc)\n",
    "        textProc = \" \".join(textProc.split())\n",
    "        return textProc\n",
    "    except: \n",
    "        return \"name error\"\n",
    "    \n",
    "def rmse(y, y0):\n",
    "    assert len(y) == len(y0)\n",
    "    return np.sqrt(np.mean(np.power((y - y0), 2)))\n",
    "\n",
    "dtypes = {\n",
    "        'category_name': 'category',\n",
    "        'parent_category_name': 'category',\n",
    "        'region': 'category',\n",
    "        'item_seq_number': 'uint32',\n",
    "        'user_type': 'category',\n",
    "        'image_top_1': 'float32',\n",
    "        'price':'float32',\n",
    "        'deal_probability': 'float32'\n",
    "        }\n",
    "print(\"\\nData Load Stage\")\n",
    "training = pd.read_csv('../data/train.csv.zip',compression='zip' ,index_col = \"item_id\", parse_dates = [\"activation_date\"],dtype=dtypes)\n",
    "traindex = training.index\n",
    "\n",
    "#training_index = df.loc[training.activation_date<=pd.to_datetime('2017-03-24')].index\n",
    "#validation_index = df.loc[training.activation_date>=pd.to_datetime('2017-03-25')].index\n",
    "testing = pd.read_csv('../data/test.csv.zip',compression='zip' ,index_col = \"item_id\", parse_dates = [\"activation_date\"],dtype=dtypes)\n",
    "testdex = testing.index\n",
    "\n",
    "ntrain = training.shape[0]\n",
    "ntest = testing.shape[0]\n",
    "y = training.deal_probability.copy()\n",
    "\n",
    "\n",
    "kf = KFold(ntrain, n_folds=NFOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "training.drop(\"deal_probability\",axis=1, inplace=True)\n",
    "print('Train shape: {} Rows, {} Columns'.format(*training.shape))\n",
    "print('Test shape: {} Rows, {} Columns'.format(*testing.shape))\n",
    "\n",
    "print(\"Combine Train and Test\")\n",
    "df = pd.concat([training,testing],axis=0)\n",
    "del training, testing\n",
    "gc.collect()\n",
    "print('\\nAll Data shape: {} Rows, {} Columns'.format(*df.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-07T12:56:50.614623Z",
     "start_time": "2018-06-07T12:56:50.580964Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>parent_category_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>param_1</th>\n",
       "      <th>param_2</th>\n",
       "      <th>param_3</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>item_seq_number</th>\n",
       "      <th>activation_date</th>\n",
       "      <th>user_type</th>\n",
       "      <th>image</th>\n",
       "      <th>image_top_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b912c3c6a6ad</th>\n",
       "      <td>e00f8ff2eaf9</td>\n",
       "      <td>Свердловская область</td>\n",
       "      <td>Екатеринбург</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Товары для детей и игрушки</td>\n",
       "      <td>Постельные принадлежности</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Кокоби(кокон для сна)</td>\n",
       "      <td>Кокон для сна малыша,пользовались меньше месяц...</td>\n",
       "      <td>400.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-03-28</td>\n",
       "      <td>Private</td>\n",
       "      <td>d10c7e016e03247a3bf2d13348fe959fe6f436c1caf64c...</td>\n",
       "      <td>1008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2dac0150717d</th>\n",
       "      <td>39aeb48f0017</td>\n",
       "      <td>Самарская область</td>\n",
       "      <td>Самара</td>\n",
       "      <td>Для дома и дачи</td>\n",
       "      <td>Мебель и интерьер</td>\n",
       "      <td>Другое</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Стойка для Одежды</td>\n",
       "      <td>Стойка для одежды, под вешалки. С бутика.</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>19</td>\n",
       "      <td>2017-03-26</td>\n",
       "      <td>Private</td>\n",
       "      <td>79c9392cc51a9c81c6eb91eceb8e552171db39d7142700...</td>\n",
       "      <td>692.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ba83aefab5dc</th>\n",
       "      <td>91e2f88dd6e3</td>\n",
       "      <td>Ростовская область</td>\n",
       "      <td>Ростов-на-Дону</td>\n",
       "      <td>Бытовая электроника</td>\n",
       "      <td>Аудио и видео</td>\n",
       "      <td>Видео, DVD и Blu-ray плееры</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Philips bluray</td>\n",
       "      <td>В хорошем состоянии, домашний кинотеатр с blu ...</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>9</td>\n",
       "      <td>2017-03-20</td>\n",
       "      <td>Private</td>\n",
       "      <td>b7f250ee3f39e1fedd77c141f273703f4a9be59db4b48a...</td>\n",
       "      <td>3032.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02996f1dd2ea</th>\n",
       "      <td>bf5cccea572d</td>\n",
       "      <td>Татарстан</td>\n",
       "      <td>Набережные Челны</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Товары для детей и игрушки</td>\n",
       "      <td>Автомобильные кресла</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Автокресло</td>\n",
       "      <td>Продам кресло от0-25кг</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>286</td>\n",
       "      <td>2017-03-25</td>\n",
       "      <td>Company</td>\n",
       "      <td>e6ef97e0725637ea84e3d203e82dadb43ed3cc0a1c8413...</td>\n",
       "      <td>796.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7c90be56d2ab</th>\n",
       "      <td>ef50846afc0b</td>\n",
       "      <td>Волгоградская область</td>\n",
       "      <td>Волгоград</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили</td>\n",
       "      <td>С пробегом</td>\n",
       "      <td>ВАЗ (LADA)</td>\n",
       "      <td>2110</td>\n",
       "      <td>ВАЗ 2110, 2003</td>\n",
       "      <td>Все вопросы по телефону.</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-03-16</td>\n",
       "      <td>Private</td>\n",
       "      <td>54a687a3a0fc1d68aed99bdaaf551c5c70b761b16fd0a2...</td>\n",
       "      <td>2264.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   user_id                 region              city  \\\n",
       "item_id                                                               \n",
       "b912c3c6a6ad  e00f8ff2eaf9   Свердловская область      Екатеринбург   \n",
       "2dac0150717d  39aeb48f0017      Самарская область            Самара   \n",
       "ba83aefab5dc  91e2f88dd6e3     Ростовская область    Ростов-на-Дону   \n",
       "02996f1dd2ea  bf5cccea572d              Татарстан  Набережные Челны   \n",
       "7c90be56d2ab  ef50846afc0b  Волгоградская область         Волгоград   \n",
       "\n",
       "             parent_category_name               category_name  \\\n",
       "item_id                                                         \n",
       "b912c3c6a6ad          Личные вещи  Товары для детей и игрушки   \n",
       "2dac0150717d      Для дома и дачи           Мебель и интерьер   \n",
       "ba83aefab5dc  Бытовая электроника               Аудио и видео   \n",
       "02996f1dd2ea          Личные вещи  Товары для детей и игрушки   \n",
       "7c90be56d2ab            Транспорт                  Автомобили   \n",
       "\n",
       "                                  param_1     param_2 param_3  \\\n",
       "item_id                                                         \n",
       "b912c3c6a6ad    Постельные принадлежности         NaN     NaN   \n",
       "2dac0150717d                       Другое         NaN     NaN   \n",
       "ba83aefab5dc  Видео, DVD и Blu-ray плееры         NaN     NaN   \n",
       "02996f1dd2ea         Автомобильные кресла         NaN     NaN   \n",
       "7c90be56d2ab                   С пробегом  ВАЗ (LADA)    2110   \n",
       "\n",
       "                              title  \\\n",
       "item_id                               \n",
       "b912c3c6a6ad  Кокоби(кокон для сна)   \n",
       "2dac0150717d      Стойка для Одежды   \n",
       "ba83aefab5dc         Philips bluray   \n",
       "02996f1dd2ea             Автокресло   \n",
       "7c90be56d2ab         ВАЗ 2110, 2003   \n",
       "\n",
       "                                                    description    price  \\\n",
       "item_id                                                                    \n",
       "b912c3c6a6ad  Кокон для сна малыша,пользовались меньше месяц...    400.0   \n",
       "2dac0150717d          Стойка для одежды, под вешалки. С бутика.   3000.0   \n",
       "ba83aefab5dc  В хорошем состоянии, домашний кинотеатр с blu ...   4000.0   \n",
       "02996f1dd2ea                             Продам кресло от0-25кг   2200.0   \n",
       "7c90be56d2ab                           Все вопросы по телефону.  40000.0   \n",
       "\n",
       "              item_seq_number activation_date user_type  \\\n",
       "item_id                                                   \n",
       "b912c3c6a6ad                2      2017-03-28   Private   \n",
       "2dac0150717d               19      2017-03-26   Private   \n",
       "ba83aefab5dc                9      2017-03-20   Private   \n",
       "02996f1dd2ea              286      2017-03-25   Company   \n",
       "7c90be56d2ab                3      2017-03-16   Private   \n",
       "\n",
       "                                                          image  image_top_1  \n",
       "item_id                                                                       \n",
       "b912c3c6a6ad  d10c7e016e03247a3bf2d13348fe959fe6f436c1caf64c...       1008.0  \n",
       "2dac0150717d  79c9392cc51a9c81c6eb91eceb8e552171db39d7142700...        692.0  \n",
       "ba83aefab5dc  b7f250ee3f39e1fedd77c141f273703f4a9be59db4b48a...       3032.0  \n",
       "02996f1dd2ea  e6ef97e0725637ea84e3d203e82dadb43ed3cc0a1c8413...        796.0  \n",
       "7c90be56d2ab  54a687a3a0fc1d68aed99bdaaf551c5c70b761b16fd0a2...       2264.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-07T13:09:17.826280Z",
     "start_time": "2018-06-07T12:56:50.618115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering\n",
      "\n",
      "Encode Variables\n",
      "Encoding : ['user_id', 'region', 'city', 'user_type', 'image_top_1']\n",
      "\n",
      "Text Features\n",
      "\n",
      "[TF-IDF] Term Frequency Inverse Document Frequency Stage\n",
      "Vectorization Runtime: 7.72 Minutes\n",
      "df shape : (2011862, 33)\n",
      "ready_df shape : (2011862, 2927)\n",
      "len vocab feature : 2927\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature Engineering\")\n",
    "df[\"price\"] = np.log(df[\"price\"]+0.001)\n",
    "df[\"price\"].fillna(-999,inplace=True)\n",
    "df[\"image_top_1\"].fillna(-999,inplace=True)\n",
    "\n",
    "#print(\"\\nCreate Time Variables\")\n",
    "df[\"Weekday\"] = df['activation_date'].dt.weekday\n",
    "df[\"Weekd of Year\"] = df['activation_date'].dt.week\n",
    "df[\"Day of Month\"] = df['activation_date'].dt.day\n",
    "dfdex = df.index\n",
    "agg_df = pd.read_csv('./aggregated_features.csv')\n",
    "df = df.merge(agg_df,on='user_id', how='left')\n",
    "df.index = dfdex\n",
    "# Create Validation Index and Remove Dead Variables\n",
    "\n",
    "df.drop([\"activation_date\",\"image\"],axis=1,inplace=True)\n",
    "\n",
    "print(\"\\nEncode Variables\")\n",
    "categorical = [\"user_id\",\"region\",\"city\",\"user_type\",\"image_top_1\"]\n",
    "print(\"Encoding :\",categorical)\n",
    "\n",
    "# Encoder:\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for col in categorical:\n",
    "    df[col] = lbl.fit_transform(df[col].astype(str))\n",
    "    \n",
    "print(\"\\nText Features\")\n",
    "\n",
    "# Feature Engineering \n",
    "df['text_feat'] = df.apply(lambda row: ' '.join([\n",
    "    str(row['param_1']), \n",
    "    str(row['param_2'])]),axis=1) # Group Param Features\n",
    "    \n",
    "df.drop([\"param_1\",\"param_2\",\"param_3\"],axis=1,inplace=True)\n",
    "\n",
    "# Meta Text Features\n",
    "textfeats = [\"description\",\"text_feat\", \"title\",\"parent_category_name\",\"category_name\"]\n",
    "\n",
    "df['title'] = df['title'].apply(lambda x: cleanName(x))\n",
    "df[\"description\"]   = df[\"description\"].apply(lambda x: cleanName(x))\n",
    "for cols in textfeats:\n",
    "    from string import digits\n",
    "    \n",
    "    df[cols] = df[cols].astype(str) \n",
    "    df[cols] = df[cols].astype(str).fillna('.') # FILL NA\n",
    "    df[cols] = df[cols].str.lower() # Lowercase all text, so that capitalized words dont get treated differently\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    df[cols] = df[cols].str.translate(remove_digits)\n",
    "    df[cols + '_num_chars'] = df[cols].apply(len) # Count number of Characters\n",
    "    df[cols + '_num_words'] = df[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n",
    "    df[cols + '_num_unique_words'] = df[cols].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df[cols + '_words_vs_unique'] = df[cols+'_num_unique_words'] / df[cols+'_num_words'] * 100 # Count Unique Words\n",
    "\n",
    "print(\"\\n[TF-IDF] Term Frequency Inverse Document Frequency Stage\")\n",
    "#russian_stop = set(stopwords.words('russian'))\n",
    "df=df.fillna(-9999)\n",
    "tfidf_para = {\n",
    "    \"stop_words\": None,\n",
    "    \"analyzer\": 'word',\n",
    "    \"token_pattern\": r'\\w{1,}',\n",
    "    \"sublinear_tf\": True,\n",
    "    \"dtype\": np.float32,\n",
    "    \"norm\": 'l2',\n",
    "    \"min_df\":50,\n",
    "    \"max_df\":.7,\n",
    "    \"smooth_idf\":False\n",
    "}\n",
    "\n",
    "\n",
    "def get_col(col_name): return lambda x: x[col_name]\n",
    "##I added to the max_features of the description. It did not change my score much but it may be worth investigating\n",
    "vectorizer = FeatureUnion([\n",
    "        ('description',TfidfVectorizer(\n",
    "            ngram_range=(1, 1),\n",
    "            max_features=1000,\n",
    "            **tfidf_para,\n",
    "            preprocessor=get_col('description'))),\n",
    "        ('text_feat',CountVectorizer(\n",
    "            ngram_range=(1, 1),\n",
    "            max_features=1000,\n",
    "            preprocessor=get_col('text_feat'))),\n",
    "        ('parent_category_name',CountVectorizer(\n",
    "            ngram_range=(1, 1),\n",
    "            max_features=1000,\n",
    "            preprocessor=get_col('parent_category_name'))),\n",
    "        ('category_name',CountVectorizer(\n",
    "            ngram_range=(1, 1),\n",
    "            max_features=1000,\n",
    "            preprocessor=get_col('category_name'))),\n",
    "        ('title',TfidfVectorizer(\n",
    "            ngram_range=(1, 1),\n",
    "            **tfidf_para,\n",
    "            max_features=1000,\n",
    "            preprocessor=get_col('title')))\n",
    "    ])\n",
    "    \n",
    "start_vect=time.time()\n",
    "\n",
    "#Fit my vectorizer on the entire dataset instead of the training rows\n",
    "#Score improved by .0001\n",
    "vectorizer.fit(df.to_dict('records'))\n",
    "\n",
    "ready_df = vectorizer.transform(df.to_dict('records'))\n",
    "tfvocab = vectorizer.get_feature_names()\n",
    "print(\"Vectorization Runtime: %0.2f Minutes\"%((time.time() - start_vect)/60))\n",
    "\n",
    "# Drop Text Cols\n",
    "df.drop(textfeats, axis=1,inplace=True)\n",
    "\n",
    "print(\"df shape :\", df.shape)\n",
    "print(\"ready_df shape :\", ready_df.shape)\n",
    "print('len vocab feature :', len(tfvocab) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-07T12:56:12.310Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18552"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "23+18529"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-07T12:56:12.313Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1503424 Rows and 2960 Cols\n",
      "508438 Rows and 2960 Cols\n",
      "Feature Names Length:  2927\n",
      "\n",
      "Fold 0\n",
      "\n",
      "Fold 1\n",
      "\n",
      "Fold 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "########################################################################################################\n",
    "# Combine Dense Features with Sparse Text Bag of Words Features\n",
    "X = hstack([csr_matrix(df.loc[traindex,:].values),ready_df[0:traindex.shape[0]]]) # Sparse Matrix\n",
    "testing = hstack([csr_matrix(df.loc[testdex,:].values),ready_df[traindex.shape[0]:]])\n",
    "#tfvocab = df.columns.tolist() + tfvocab\n",
    "for shape in [X,testing]:\n",
    "    print(\"{} Rows and {} Cols\".format(*shape.shape))\n",
    "print(\"Feature Names Length: \",len(tfvocab))\n",
    "################################################################################################\n",
    "ridge_params = {'alpha':30.0, 'fit_intercept':True, 'normalize':False, 'copy_X':True,\n",
    "                'max_iter':None, 'tol':0.0025, 'solver':'auto', 'random_state':SEED}\n",
    "\n",
    "#Ridge oof method from Faron's kernel\n",
    "#I was using this to analyze my vectorization, but figured it would be interesting to add the results back into the dataset\n",
    "#It doesn't really add much to the score, but it does help lightgbm converge faster\n",
    "ridge = SklearnWrapper(clf=Ridge, seed = SEED, params = ridge_params)\n",
    "ridge_oof_train, ridge_oof_test = get_oof(ridge, X, y, testing)\n",
    "\n",
    "rms = sqrt(mean_squared_error(y, ridge_oof_train))\n",
    "print('Ridge OOF RMSE: {}'.format(rms))\n",
    "\n",
    "print(\"Modeling Stage\")\n",
    "\n",
    "ridge_preds = np.concatenate([ridge_oof_train, ridge_oof_test])\n",
    "print(ridge_preds.shape)\n",
    "df['ridge_preds'] = ridge_preds\n",
    "########################################################################################################\n",
    "\n",
    "########################################################################################################\n",
    "print(\"Light Gradient Boosting Regressor\")\n",
    "lgbm_pa =  {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'n_estimators': 100,\n",
    "    'num_leaves': 450,\n",
    "    'max_depth': 15,\n",
    "    'subsample' : 0.8,\n",
    "    'learning_rate': 0.02,\n",
    "    'colsample_bytree': 0.65\n",
    "                }  \n",
    "'''lgbm_params =  ( boosting_type='gbdt', colsample_bytree=1, learning_rate=0.1,\n",
    "       max_bin=255, max_depth=-1, min_child_samples=10, min_child_weight=5,\n",
    "       min_split_gain=0, n_estimators=10, nthread=-1, num_leaves=31,\n",
    "       objective='regression', reg_alpha=0, reg_lambda=0, seed=0,\n",
    "       silent=True, subsample=1, subsample_for_bin=50000, subsample_freq=1)'''\n",
    "\n",
    "\n",
    "lgbm = SklearnWrapper(clf=lgb.LGBMRegressor, seed = SEED, params = lgbm_pa,lgbm=True)\n",
    "\n",
    "lgbm_oof_train, lgbm_oof_test = get_oof(lgbm, X, y, testing)\n",
    "\n",
    "rms = sqrt(mean_squared_error(y, lgbm_oof_train))\n",
    "print('lgbm OOF RMSE: {}'.format(rms))\n",
    "\n",
    "print(\"Modeling Stage\")\n",
    "\n",
    "lgb_preds = np.concatenate([lgbm_oof_train, lgbm_oof_test])\n",
    "print(lgb_preds.shape)\n",
    "df['lgb_preds'] = lgb_preds\n",
    "########################################################################################################\n",
    "print(\"Light Gradient Boosting Regressor\")\n",
    "lgbm_pa =  {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'n_estimators': 300,\n",
    "    'num_leaves': 650,\n",
    "    'max_depth': 20,\n",
    "    'subsample' : 0.8,\n",
    "    'learning_rate': 0.02,\n",
    "    'colsample_bytree': 0.9\n",
    "                }  \n",
    "'''lgbm_params =  ( boosting_type='gbdt', colsample_bytree=1, learning_rate=0.1,\n",
    "       max_bin=255, max_depth=-1, min_child_samples=10, min_child_weight=5,\n",
    "       min_split_gain=0, n_estimators=10, nthread=-1, num_leaves=31,\n",
    "       objective='regression', reg_alpha=0, reg_lambda=0, seed=0,\n",
    "       silent=True, subsample=1, subsample_for_bin=50000, subsample_freq=1)'''\n",
    "\n",
    "\n",
    "lgbm = SklearnWrapper(clf=lgb.LGBMRegressor, seed = SEED, params = lgbm_pa,lgbm=True)\n",
    "\n",
    "lgbm_oof_train, lgbm_oof_test = get_oof(lgbm, X, y, testing)\n",
    "\n",
    "rms = sqrt(mean_squared_error(y, lgbm_oof_train))\n",
    "print('lgbm OOF RMSE: {}'.format(rms))\n",
    "\n",
    "print(\"Modeling Stage\")\n",
    "\n",
    "lgb_preds = np.concatenate([lgbm_oof_train, lgbm_oof_test])\n",
    "print(lgb_preds.shape)\n",
    "df['lgb_preds_2'] = lgb_preds\n",
    "########################################################################################################\n",
    "print(\"Light Gradient Boosting Regressor\")\n",
    "lgbm_pa =  {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'n_estimators': 200,\n",
    "    'num_leaves': 850,\n",
    "    #'max_depth': 15,\n",
    "    'subsample' : 0.8,\n",
    "    'learning_rate': 0.02,\n",
    "    'colsample_bytree': 0.5\n",
    "                }  \n",
    "'''lgbm_params =  ( boosting_type='gbdt', colsample_bytree=1, learning_rate=0.1,\n",
    "       max_bin=255, max_depth=-1, min_child_samples=10, min_child_weight=5,\n",
    "       min_split_gain=0, n_estimators=10, nthread=-1, num_leaves=31,\n",
    "       objective='regression', reg_alpha=0, reg_lambda=0, seed=0,\n",
    "       silent=True, subsample=1, subsample_for_bin=50000, subsample_freq=1)'''\n",
    "\n",
    "\n",
    "lgbm = SklearnWrapper(clf=lgb.LGBMRegressor, seed = SEED, params = lgbm_pa,lgbm=True)\n",
    "\n",
    "lgbm_oof_train, lgbm_oof_test = get_oof(lgbm, X, y, testing)\n",
    "\n",
    "rms = sqrt(mean_squared_error(y, lgbm_oof_train))\n",
    "print('lgbm OOF RMSE: {}'.format(rms))\n",
    "\n",
    "print(\"Modeling Stage\")\n",
    "\n",
    "lgb_preds = np.concatenate([lgbm_oof_train, lgbm_oof_test])\n",
    "print(lgb_preds.shape)\n",
    "df['lgb_preds_3'] = lgb_preds\n",
    "########################################################################################################\n",
    "print(\"Light Gradient Boosting Regressor\")\n",
    "lgbm_pa =  {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'n_estimators': 700,\n",
    "    'num_leaves': 320,\n",
    "    'max_depth': 15,\n",
    "    'subsample' : 0.95,\n",
    "    'learning_rate': 0.02,\n",
    "    'colsample_bytree': 0.95\n",
    "                }  \n",
    "'''lgbm_params =  ( boosting_type='gbdt', colsample_bytree=1, learning_rate=0.1,\n",
    "       max_bin=255, max_depth=-1, min_child_samples=10, min_child_weight=5,\n",
    "       min_split_gain=0, n_estimators=10, nthread=-1, num_leaves=31,\n",
    "       objective='regression', reg_alpha=0, reg_lambda=0, seed=0,\n",
    "       silent=True, subsample=1, subsample_for_bin=50000, subsample_freq=1)'''\n",
    "\n",
    "\n",
    "lgbm = SklearnWrapper(clf=lgb.LGBMRegressor, seed = SEED, params = lgbm_pa,lgbm=True)\n",
    "\n",
    "lgbm_oof_train, lgbm_oof_test = get_oof(lgbm, X, y, testing)\n",
    "\n",
    "rms = sqrt(mean_squared_error(y, lgbm_oof_train))\n",
    "print('lgbm OOF RMSE: {}'.format(rms))\n",
    "\n",
    "print(\"Modeling Stage\")\n",
    "\n",
    "lgb_preds = np.concatenate([lgbm_oof_train, lgbm_oof_test])\n",
    "print(lgb_preds.shape)\n",
    "df['lgb_preds_4'] = lgb_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-07T12:56:12.319Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df['preds_diff'] = df['lgb_preds'] - df['ridge_preds']\n",
    "#df['preds_sum'] = 0.8*df['lgb_preds'] + 0.2*df['ridge_preds']\n",
    "#df['predslgb_sum'] = 0.25*df['lgb_preds'] + 0.25*df['lgb_preds_2']+0.25*df['lgb_preds_3']+0.25*df['lgb_preds_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-07T12:56:12.322Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-07T12:56:12.326Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "# Combine Dense Features with Sparse Text Bag of Words Features\n",
    "X = hstack([csr_matrix(df.loc[traindex,:].values),ready_df[0:traindex.shape[0]]]) # Sparse Matrix\n",
    "testing = hstack([csr_matrix(df.loc[testdex,:].values),ready_df[traindex.shape[0]:]])\n",
    "tfvocab = df.columns.tolist() + tfvocab\n",
    "for shape in [X,testing]:\n",
    "    print(\"{} Rows and {} Cols\".format(*shape.shape))\n",
    "print(\"Feature Names Length: \",len(tfvocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-07T12:56:12.332Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#del df\n",
    "#gc.collect();\n",
    "#X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=230)\n",
    "#for shape in [X_train, X_valid]:\n",
    "#    print(\"{} Rows and {} Cols\".format(*shape.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-07T12:56:12.336Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-07T12:56:12.339Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfvocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-07T12:56:12.342Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "# Combine Dense Features with Sparse Text Bag of Words Featur\n",
    "\n",
    "print(\"Light Gradient Boosting Regressor\")\n",
    "lgbm_params =  {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'max_depth': 15,\n",
    "    #'num_leaves': 32,\n",
    "    'feature_fraction': 0.7,\n",
    "    'bagging_fraction': 0.9,\n",
    "    'bagging_freq': 2,\n",
    "    'learning_rate': 0.02,\n",
    "    'verbose': 0\n",
    "}  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pred_test_full =0\n",
    "cv_score = []\n",
    "NFOLDS=10\n",
    "kf = KFold(ntrain, n_folds=NFOLDS, shuffle=True, random_state=SEED)\n",
    "for i, (train_index, test_index) in enumerate(kf):\n",
    "    print('\\nFold {}'.format(i))\n",
    "\n",
    "    xtr,xvl = X.tocsr()[train_index],X.tocsr()[test_index]\n",
    "    ytr,yvl = y[train_index],y[test_index]\n",
    "    \n",
    "    \n",
    "    lgtrain = lgb.Dataset(xtr, ytr,\n",
    "                feature_name=tfvocab,\n",
    "                categorical_feature = categorical)\n",
    "    lgvalid = lgb.Dataset(xvl, yvl,\n",
    "                feature_name=tfvocab,\n",
    "                categorical_feature = categorical)\n",
    "\n",
    "    modelstart = time.time()\n",
    "    lgb_clf = lgb.train(\n",
    "        lgbm_params,\n",
    "        lgtrain,\n",
    "        num_boost_round=16000,\n",
    "        valid_sets=[lgtrain, lgvalid],\n",
    "        valid_names=['train','valid'],\n",
    "        early_stopping_rounds=150,\n",
    "        verbose_eval=100\n",
    "    )\n",
    "    pred_test = lgb_clf.predict(testing) \n",
    "    pred_test_full += pred_test\n",
    "    cv_score.append(lgb_clf.best_score)\n",
    "\n",
    "\n",
    "pred_lgb = pred_test_full/NFOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-07T12:56:12.345Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "Dparam = {'objective' : \"reg:logistic\",\n",
    "          'booster' : \"gbtree\",\n",
    "          'eval_metric' : \"rmse\",\n",
    "          'nthread' : 8,\n",
    "          'eta':0.07,\n",
    "          'max_depth':18,\n",
    "          'min_child_weight': 2,\n",
    "          'gamma' :0,\n",
    "          'subsample':0.7,\n",
    "          'colsample_bytree':0.7,\n",
    "          'aplha':0,\n",
    "          'lambda':0,\n",
    "          'nrounds' : 1700}  \n",
    "\n",
    "pred_test_full_xgb=0\n",
    "cv_score_xgb = []\n",
    "NFOLDS=10\n",
    "kf = KFold(ntrain, n_folds=NFOLDS, shuffle=True, random_state=SEED)\n",
    "for i, (train_index, test_index) in enumerate(kf):\n",
    "    print('\\nFold {}'.format(i))\n",
    "    xtr,xvl = X.tocsr()[train_index],X.tocsr()[test_index]\n",
    "    ytr,yvl = y[train_index],y[test_index]\n",
    "    dtrain =xgb.DMatrix(data = xtr, label = ytr)\n",
    "    dval =xgb.DMatrix(data = xvl, label = yvl)\n",
    "    watchlist = [(dval, 'eval')]\n",
    "    print(\"Training Model\")\n",
    "    m_xgb=xgb.train(params=Dparam,dtrain=dtrain,\n",
    "                    num_boost_round=Dparam['nrounds'],\n",
    "                    early_stopping_rounds=100,evals=watchlist)\n",
    "    dtest = xgb.DMatrix(data = testing)\n",
    "    pred_test_xgb = m_xgb.predict(dtest) \n",
    "    pred_test_full_xgb += pred_test_xgb\n",
    "    cv_score_xgb.append(m_xgb.best_score)\n",
    "\n",
    "\n",
    "pred_xgb = pred_test_full_xgb/NFOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-07T12:56:12.348Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:23:33.422447Z",
     "start_time": "2018-06-05T12:22:18.909Z"
    }
   },
   "source": [
    "\n",
    "print(\"Light Gradient Boosting Regressor\")\n",
    "lgbm_params =  {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    #'max_depth': 15,\n",
    "    'num_leaves': 250,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 2,\n",
    "    'learning_rate': 0.05,\n",
    "    'verbose': 0\n",
    "}  \n",
    "\n",
    "\n",
    "lgtrain = lgb.Dataset(X_train, y_train,\n",
    "                feature_name=tfvocab)\n",
    "lgvalid = lgb.Dataset(X_valid, y_valid,\n",
    "                feature_name=tfvocab)\n",
    "\n",
    "modelstart = time.time()\n",
    "lgb_clf = lgb.train(\n",
    "    lgbm_params,\n",
    "    lgtrain,\n",
    "    num_boost_round=16000,\n",
    "    valid_sets=[lgtrain, lgvalid],\n",
    "    valid_names=['train','valid'],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T21:53:09.331086Z",
     "start_time": "2018-06-06T21:53:03.614685Z"
    }
   },
   "source": [
    "# Feature Importance Plot\n",
    "f, ax = plt.subplots(figsize=[7,10])\n",
    "lgb.plot_importance(lgb_clf, max_num_features=50, ax=ax)\n",
    "plt.title(\"Light GBM Feature Importance\")\n",
    "plt.savefig('feature_import.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-07T12:56:12.442Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Model Evaluation Stage\")\n",
    "#print('RMSE:', np.sqrt(metrics.mean_squared_error(y_valid, lgb_clf.predict(X_valid))))\n",
    "#lgpred = lgb_clf.predict(testing) \n",
    "lgpred = pred_lgb\n",
    "#Mixing lightgbm with ridge. I haven't really tested if this improves the score or not\n",
    "#blend = 0.95*lgpred + 0.05*ridge_oof_test[:,0]\n",
    "lgsub = pd.DataFrame(lgpred,columns=[\"deal_probability\"],index=testdex)\n",
    "lgsub['deal_probability'].clip(0.0, 1.0, inplace=True) # Between 0 and 1\n",
    "lgsub.to_csv(\"lgsub_Stacking_CV_5.csv\",index=True,header=True)\n",
    "print(\"Model Runtime: %0.2f Minutes\"%((time.time() - modelstart)/60))\n",
    "print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-07T12:56:12.445Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgtrain.save_binary('trainwithouttxt.bin')\n",
    "lgvalid.save_binary('validwithouttxt.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-07T12:56:12.448Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Model Evaluation Stage\")\n",
    "#print('RMSE:', np.sqrt(metrics.mean_squared_error(y_valid, lgb_clf.predict(X_valid))))\n",
    "#lgpred = lgb_clf.predict(testing) \n",
    "xgpred = pred_xgb\n",
    "#Mixing lightgbm with ridge. I haven't really tested if this improves the score or not\n",
    "#blend = 0.95*lgpred + 0.05*ridge_oof_test[:,0]\n",
    "xgsub = pd.DataFrame(xgpred,columns=[\"deal_probability\"],index=testdex)\n",
    "xgsub['deal_probability'].clip(0.0, 1.0, inplace=True) # Between 0 and 1\n",
    "xgsub.to_csv(\"xgp_Stacking_CV_5.csv\",index=True,header=True)\n",
    "print(\"Model Runtime: %0.2f Minutes\"%((time.time() - modelstart)/60))\n",
    "print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-07T12:56:12.452Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Model Evaluation Stage\")\n",
    "#print('RMSE:', np.sqrt(metrics.mean_squared_error(y_valid, lgb_clf.predict(X_valid))))\n",
    "#lgpred = lgb_clf.predict(testing) \n",
    "xgpred = 0.5*pred_xgb + 0.5*lgpred\n",
    "#Mixing lightgbm with ridge. I haven't really tested if this improves the score or not\n",
    "#blend = 0.95*lgpred + 0.05*ridge_oof_test[:,0]\n",
    "xgsub = pd.DataFrame(xgpred,columns=[\"deal_probability\"],index=testdex)\n",
    "xgsub['deal_probability'].clip(0.0, 1.0, inplace=True) # Between 0 and 1\n",
    "xgsub.to_csv(\"xgp_lgb_Stacking_CV_5.csv\",index=True,header=True)\n",
    "print(\"Model Runtime: %0.2f Minutes\"%((time.time() - modelstart)/60))\n",
    "print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-07T12:56:12.455Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"ALL done ....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
