{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T17:10:44.607530Z",
     "start_time": "2018-04-25T17:10:44.602721Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T17:10:44.670677Z",
     "start_time": "2018-04-25T17:10:44.609376Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_count( df, group_cols, agg_name, agg_type='uint32', show_max=False, show_agg=True ):\n",
    "    if show_agg:\n",
    "        print( \"Aggregating by \", group_cols , '...' )\n",
    "    gp = df[group_cols][group_cols].groupby(group_cols).size().rename(agg_name).to_frame().reset_index()\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "    gc.collect()\n",
    "    return( df )\n",
    "\n",
    "def do_countuniq( df, group_cols, counted, agg_name, agg_type='uint32', show_max=False, show_agg=True ):\n",
    "    if show_agg:\n",
    "        print( \"Counting unqiue \", counted, \" by \", group_cols , '...' )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].nunique().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "    gc.collect()\n",
    "    return( df )\n",
    "    \n",
    "def do_cumcount( df, group_cols, counted, agg_name, agg_type='uint32', show_max=False, show_agg=True ):\n",
    "    if show_agg:\n",
    "        print( \"Cumulative count by \", group_cols , '...' )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].cumcount()\n",
    "    df[agg_name]=gp.values\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "    gc.collect()\n",
    "    return( df )\n",
    "\n",
    "def do_mean( df, group_cols, counted, agg_name, agg_type='float32', show_max=False, show_agg=True ):\n",
    "    if show_agg:\n",
    "        print( \"Calculating mean of \", counted, \" by \", group_cols , '...' )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].mean().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "    gc.collect()\n",
    "    return( df )\n",
    "\n",
    "def do_var( df, group_cols, counted, agg_name, agg_type='float32', show_max=False, show_agg=True ):\n",
    "    if show_agg:\n",
    "        print( \"Calculating variance of \", counted, \" by \", group_cols , '...' )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].var().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "    gc.collect()\n",
    "    return( df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T17:10:44.739374Z",
     "start_time": "2018-04-25T17:10:44.672709Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lgb_modelfit_nocv(params, dtrain, dvalid, predictors, target='target', objective='binary', metrics='auc',\n",
    "                 feval=None, early_stopping_rounds=20, num_boost_round=3000, verbose_eval=10, categorical_features=None):\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': objective,\n",
    "        'metric':metrics,\n",
    "        'learning_rate': 0.2,\n",
    "        #'is_unbalance': 'true',  #because training data is unbalance (replaced with scale_pos_weight)\n",
    "        'num_leaves': 31,  # we should let it be smaller than 2^(max_depth)\n",
    "        'max_depth': -1,  # -1 means no limit\n",
    "        'min_child_samples': 20,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'max_bin': 255,  # Number of bucketed bin for feature values\n",
    "        'subsample': 0.6,  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': 0,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.3,  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 5,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "        'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "        'reg_alpha': 0,  # L1 regularization term on weights\n",
    "        'reg_lambda': 0,  # L2 regularization term on weights\n",
    "        'nthread': 10,\n",
    "        'verbose': 0,\n",
    "        'metric':metrics\n",
    "    }\n",
    "\n",
    "    lgb_params.update(params)\n",
    "    ################################################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #################################################################################################################\n",
    "\n",
    "    print(\"preparing validation datasets\")\n",
    "\n",
    "    xgtrain = lgb.Dataset(dtrain[predictors].values, label=dtrain[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "    #xgvalid = lgb.Dataset(dvalid[predictors].values, label=dvalid[target].values,\n",
    "    #                      feature_name=predictors,\n",
    "    #                       categorical_feature=categorical_features\n",
    "    #                      )\n",
    "\n",
    "    evals_results = {}\n",
    "    ########################################################################################################################\n",
    "    \n",
    "    from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "    from hyperopt.fmin import fmin\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    # objective function to optimize; loss is auroc\n",
    "    \n",
    "\n",
    "    def objective(params):\n",
    "\n",
    "        bst = lgb.train(params, \n",
    "                         xgtrain, \n",
    "                         valid_sets= [xgtrain],  \n",
    "                         num_boost_round=200,\n",
    "                         #early_stopping_rounds=30,\n",
    "                         verbose_eval=20)\n",
    "    \n",
    "        pred = bst.predict(dvalid[predictors])\n",
    "        auc = roc_auc_score(dvalid[target], pred)\n",
    "\n",
    "        del bst, pred\n",
    "        gc.collect()\n",
    "        print('**********************************************************')\n",
    "        print(params)\n",
    "        print(\"SCORE ............. : \",auc)\n",
    "        print('**********************************************************')\n",
    "        \n",
    "        return { 'loss': 1-auc, 'status': STATUS_OK }\n",
    "\n",
    "    space = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "            'metric':'auc',\n",
    "            'learning_rate': 0.2,\n",
    "            'num_leaves': hp.choice('num_leaves', np.arange(7, 500,10, dtype=int)),\n",
    "            'max_depth': hp.choice('max_depth', np.arange(3, 12, dtype=int)),\n",
    "            'min_child_samples': hp.choice('min_child_samples', np.arange(10, 500,50, dtype=int)),\n",
    "            'max_bin': hp.choice('max_bin', np.arange(100, 5000,50, dtype=int)),\n",
    "            #\"drop_rate\": 0.2,\n",
    "            'subsample': 0.7,\n",
    "            'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "            'colsample_bytree': hp.quniform('colsample_bytree', 0.2, 1, 0.1),\n",
    "            'min_child_weight': hp.quniform('min_child_weight', 1, 100, 1),  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "            'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "            'nthread': 10,\n",
    "            'verbose': 0,\n",
    "            'scale_pos_weight': hp.choice('scale_pos_weight', np.arange(100, 500,10, dtype=int)),\n",
    "            }\n",
    "\n",
    "    trials = Trials()\n",
    "    best = fmin(\n",
    "        fn=objective,\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=300, # WARNING: increase number of evaluations (it's small for the sake of example)\n",
    "        trials=trials\n",
    "        )\n",
    "\n",
    "    # best hyperparameters\n",
    "    print(\"\\n\\n\\n The best hyperparameters:\")\n",
    "    print(best)\n",
    "    #########################################################################################################################\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T17:15:05.058278Z",
     "start_time": "2018-04-25T17:10:44.741254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train data...\n",
      "loading test data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'app'           : 'uint16',\n",
    "        'device'        : 'uint16',\n",
    "        'os'            : 'uint16',\n",
    "        'channel'       : 'uint16',\n",
    "        'is_attributed' : 'uint8',\n",
    "        'click_id'      : 'uint32',\n",
    "        }\n",
    "\n",
    "print('loading train data...')\n",
    "train_df = pd.read_csv(\"./train.csv.zip\",compression='zip',skiprows=range(1,40000000) ,parse_dates=['click_time'], dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'is_attributed'])\n",
    "#######################################################\n",
    "\n",
    "\n",
    "\n",
    "#######################################################\n",
    "print('loading test data...')\n",
    "\n",
    "test_df = pd.read_csv(\"./test.csv.zip\", compression='zip',parse_dates=['click_time'], dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id'])\n",
    "\n",
    "len_train = len(train_df)\n",
    "train_df=train_df.append(test_df)\n",
    "del test_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T17:17:49.019811Z",
     "start_time": "2018-04-25T17:15:05.060865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Calculating confidence-weighted rate for: ['app'].\n",
      "   Saving to: app_confRate. Group Max /Mean / Median / Min: 26562016 / 219219.2 / 29.0 / 1\n",
      ">> Calculating confidence-weighted rate for: ['app', 'channel'].\n",
      "   Saving to: app_channel_confRate. Group Max /Mean / Median / Min: 11318288 / 108705.09 / 75.0 / 1\n",
      ">> Calculating confidence-weighted rate for: ['app', 'device'].\n",
      "   Saving to: app_device_confRate. Group Max /Mean / Median / Min: 25583655 / 13395.94 / 2.0 / 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_id</th>\n",
       "      <th>click_time</th>\n",
       "      <th>device</th>\n",
       "      <th>ip</th>\n",
       "      <th>is_attributed</th>\n",
       "      <th>os</th>\n",
       "      <th>app_confRate</th>\n",
       "      <th>app_channel_confRate</th>\n",
       "      <th>app_device_confRate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>477</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-11-07 09:39:40</td>\n",
       "      <td>1</td>\n",
       "      <td>147296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>145</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-11-07 09:39:40</td>\n",
       "      <td>1</td>\n",
       "      <td>48281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.001768</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.001951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>153</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-11-07 09:39:40</td>\n",
       "      <td>1</td>\n",
       "      <td>5348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.001738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-11-07 09:39:40</td>\n",
       "      <td>1</td>\n",
       "      <td>118229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-11-07 09:39:40</td>\n",
       "      <td>1</td>\n",
       "      <td>67836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   app  channel  click_id          click_time  device      ip  is_attributed  \\\n",
       "0   13      477       NaN 2017-11-07 09:39:40       1  147296            0.0   \n",
       "1    8      145       NaN 2017-11-07 09:39:40       1   48281            0.0   \n",
       "2   27      153       NaN 2017-11-07 09:39:40       1    5348            0.0   \n",
       "3   26      121       NaN 2017-11-07 09:39:40       1  118229            0.0   \n",
       "4    2      219       NaN 2017-11-07 09:39:40       1   67836            0.0   \n",
       "\n",
       "   os  app_confRate  app_channel_confRate  app_device_confRate  \n",
       "0  18      0.000161              0.000168             0.000165  \n",
       "1  19      0.001768              0.001661             0.001951  \n",
       "2  47      0.001729              0.000338             0.001738  \n",
       "3  18      0.000417              0.000350             0.000418  \n",
       "4   6      0.000244              0.000309             0.000255  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATTRIBUTION_CATEGORIES = [        \n",
    "    # V1 Features #\n",
    "    ###############\n",
    "    ['app'],\n",
    "    \n",
    "    # V2 Features #\n",
    "    ###############\n",
    "    ['app', 'channel'],\n",
    "    ['app', 'os'],\n",
    "    \n",
    "    # V3 Features #\n",
    "    ###############\n",
    "    ['os', 'device'],\n",
    "\n",
    "]\n",
    "# Find frequency of is_attributed for each unique value in column\n",
    "freqs = {}\n",
    "for cols in ATTRIBUTION_CATEGORIES:\n",
    "    \n",
    "    # New feature name\n",
    "    new_feature = '_'.join(cols)+'_confRate'    \n",
    "    \n",
    "    # Perform the groupby\n",
    "    group_object = train_df[:len_train].groupby(cols)\n",
    "    \n",
    "    # Group sizes    \n",
    "    group_sizes = group_object.size()\n",
    "    log_group = np.log(1000000) # 1000 views -> 60% confidence, 100 views -> 40% confidence \n",
    "    print(\">> Calculating confidence-weighted rate for: {}.\\n   Saving to: {}. Group Max /Mean / Median / Min: {} / {} / {} / {}\".format(\n",
    "        cols, new_feature, \n",
    "        group_sizes.max(), \n",
    "        np.round(group_sizes.mean(), 2),\n",
    "        np.round(group_sizes.median(), 2),\n",
    "        group_sizes.min()\n",
    "    ))\n",
    "    \n",
    "    # Aggregation function\n",
    "    def rate_calculation(x):\n",
    "        \"\"\"Calculate the attributed rate. Scale by confidence\"\"\"\n",
    "        rate = x.sum() / float(x.count())\n",
    "        conf = np.min([1, np.log(x.count()) / log_group])\n",
    "        return rate * conf\n",
    "    \n",
    "    # Perform the merge\n",
    "    train_df = train_df.merge(\n",
    "        group_object['is_attributed']. \\\n",
    "            apply(rate_calculation). \\\n",
    "            reset_index(). \\\n",
    "            rename( \n",
    "                index=str,\n",
    "                columns={'is_attributed': new_feature}\n",
    "            )[cols + [new_feature]],\n",
    "        on=cols, how='left'\n",
    "    )\n",
    "    \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T17:53:12.081065Z",
     "start_time": "2018-04-25T17:17:49.023258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting new features...\n",
      "('Cumulative count by ', ['ip', 'device', 'os'], '...')\n",
      "('V_2 max value = ', 248448)\n",
      "('Counting unqiue ', 'channel', ' by ', ['app'], '...')\n",
      "('V_3 max value = ', 48)\n",
      "('Counting unqiue ', 'app', ' by ', ['channel'], '...')\n",
      "('V_4 max value = ', 289)\n",
      "('Counting unqiue ', 'app', ' by ', ['ip'], '...')\n",
      "('V_7 max value = ', 263)\n",
      "('Counting unqiue ', 'channel', ' by ', ['ip'], '...')\n",
      "('V_15 max value = ', 162)\n",
      "('Aggregating by ', ['ip', 'day', 'hour'], '...')\n",
      "('V_10 max value = ', 44259)\n",
      "('Aggregating by ', ['app', 'day', 'hour'], '...')\n",
      "('V_18 max value = ', 895281)\n",
      "('Aggregating by ', ['channel', 'day', 'hour'], '...')\n",
      "('V_19 max value = ', 526788)\n",
      "('Aggregating by ', ['os', 'day', 'hour'], '...')\n",
      "('V_20 max value = ', 984911)\n",
      "('Aggregating by ', ['ip', 'app'], '...')\n",
      "('V_11 max value = ', 192442)\n",
      "('Aggregating by ', ['ip', 'app', 'os'], '...')\n",
      "('V_12 max value = ', 46279)\n",
      "('Calculating variance of ', 'channel', ' by ', ['day', 'hour', 'app'], '...')\n",
      "('V_13 max value = ', 76050.0)\n",
      "('Calculating variance of ', 'app', ' by ', ['day', 'hour', 'channel'], '...')\n",
      "('V_14 max value = ', 204800.0)\n"
     ]
    }
   ],
   "source": [
    "print('Extracting new features...')\n",
    "\n",
    "train_df['hour'] = pd.to_datetime(train_df.click_time).dt.hour.astype('uint8');gc.collect()\n",
    "train_df['min'] = pd.to_datetime(train_df.click_time).dt.minute.astype('uint8');gc.collect()\n",
    "train_df['sec'] = pd.to_datetime(train_df.click_time).dt.second.astype('uint8');gc.collect()\n",
    "train_df['day'] = pd.to_datetime(train_df.click_time).dt.day.astype('uint8');gc.collect()\n",
    "\n",
    "train_df['click_time'] = (train_df['click_time'].astype(np.int64) // 10 ** 9).astype(np.int32)\n",
    "train_df['nextClick'] = (train_df.groupby(['ip', 'app', 'device', 'os']).click_time.shift(-1) - train_df.click_time).astype(np.float32)\n",
    "#train_df['nextClick_2'] = (train_df.groupby(['device', 'os','channel','app']).click_time.shift(-1) - train_df.click_time).astype(np.float32)\n",
    "train_df.drop(['click_time'],axis=1,inplace=True)\n",
    "#train_df = do_cumcount( train_df, ['ip', 'device', 'os'], 'channel', 'V_1', show_max=True ); gc.collect()\n",
    "train_df = do_cumcount( train_df, ['ip', 'device', 'os'], 'app', 'V_2', show_max=True ); gc.collect()\n",
    "# to do ip channel count unique\n",
    "train_df = do_countuniq( train_df, ['app'], 'channel', 'V_3', show_max=True ); gc.collect()\n",
    "train_df = do_countuniq( train_df, ['channel'], 'app', 'V_4', show_max=True ); gc.collect()\n",
    "#train_df = do_countuniq( train_df, ['day','hour','channel'], 'app', 'V_5', 'uint8', show_max=True ); gc.collect()\n",
    "#train_df = do_countuniq( train_df, ['day','hour','app'], 'channel', 'V_6', 'uint8', show_max=True ); gc.collect()\n",
    "train_df = do_countuniq( train_df, ['ip'], 'app', 'V_7', 'uint8', show_max=True ); gc.collect()\n",
    "train_df = do_countuniq( train_df, ['ip'], 'channel', 'V_15', 'uint8', show_max=True ); gc.collect()\n",
    "#train_df = do_countuniq( train_df, ['ip', 'app'], 'os', 'V_8', 'uint8', show_max=True ); gc.collect()\n",
    "#train_df = do_countuniq( train_df, ['ip'], 'device', 'V_9', 'uint16', show_max=True ); gc.collect()\n",
    "\n",
    "train_df = do_count( train_df, ['ip', 'day', 'hour'], 'V_10', show_max=True ); gc.collect()\n",
    "train_df = do_count( train_df, ['app', 'day', 'hour'], 'V_18', show_max=True ); gc.collect()\n",
    "train_df = do_count( train_df, ['channel', 'day', 'hour'], 'V_19', show_max=True ); gc.collect()\n",
    "train_df = do_count( train_df, ['os', 'day', 'hour'], 'V_20', show_max=True ); gc.collect()\n",
    "train_df = do_count( train_df, ['ip', 'app'], 'V_11', show_max=True ); gc.collect()\n",
    "train_df = do_count( train_df, ['ip', 'app', 'os'], 'V_12', 'uint16', show_max=True ); gc.collect()\n",
    "\n",
    "train_df = do_var( train_df, ['day', 'hour','app'],'channel' , 'V_13', show_max=True ); gc.collect()\n",
    "train_df = do_var( train_df, ['day', 'hour', 'channel'], 'app' , 'V_14', show_max=True ); gc.collect()\n",
    "#train_df = do_var( train_df, ['day', 'hour', 'os'], 'app' , 'V_16', show_max=True ); gc.collect()\n",
    "#train_df = do_var( train_df, ['day', 'hour', 'os'], 'channel' , 'V_17', show_max=True ); gc.collect()\n",
    "train_df.drop(['ip'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T17:53:12.120740Z",
     "start_time": "2018-04-25T17:53:12.084300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_id</th>\n",
       "      <th>device</th>\n",
       "      <th>is_attributed</th>\n",
       "      <th>os</th>\n",
       "      <th>app_confRate</th>\n",
       "      <th>app_channel_confRate</th>\n",
       "      <th>app_device_confRate</th>\n",
       "      <th>hour</th>\n",
       "      <th>...</th>\n",
       "      <th>V_7</th>\n",
       "      <th>V_15</th>\n",
       "      <th>V_10</th>\n",
       "      <th>V_18</th>\n",
       "      <th>V_19</th>\n",
       "      <th>V_20</th>\n",
       "      <th>V_11</th>\n",
       "      <th>V_12</th>\n",
       "      <th>V_13</th>\n",
       "      <th>V_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>477</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>31514</td>\n",
       "      <td>32252</td>\n",
       "      <td>49232</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1181.209595</td>\n",
       "      <td>29.850218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>145</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.001768</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>74</td>\n",
       "      <td>23</td>\n",
       "      <td>16197</td>\n",
       "      <td>13968</td>\n",
       "      <td>239249</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1474.756592</td>\n",
       "      <td>1.926109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>153</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.001738</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>162</td>\n",
       "      <td>7047</td>\n",
       "      <td>7930</td>\n",
       "      <td>32683</td>\n",
       "      <td>8575</td>\n",
       "      <td>9025</td>\n",
       "      <td>204</td>\n",
       "      <td>293.065765</td>\n",
       "      <td>89.350601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>139</td>\n",
       "      <td>142</td>\n",
       "      <td>346</td>\n",
       "      <td>15335</td>\n",
       "      <td>26217</td>\n",
       "      <td>49232</td>\n",
       "      <td>1247</td>\n",
       "      <td>42</td>\n",
       "      <td>5029.855469</td>\n",
       "      <td>14.620737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>44</td>\n",
       "      <td>89</td>\n",
       "      <td>4</td>\n",
       "      <td>110713</td>\n",
       "      <td>14359</td>\n",
       "      <td>25976</td>\n",
       "      <td>138</td>\n",
       "      <td>2</td>\n",
       "      <td>19382.593750</td>\n",
       "      <td>27.856543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   app  channel  click_id  device  is_attributed  os  app_confRate  \\\n",
       "0   13      477       NaN       1            0.0  18      0.000161   \n",
       "1    8      145       NaN       1            0.0  19      0.001768   \n",
       "2   27      153       NaN       1            0.0  47      0.001729   \n",
       "3   26      121       NaN       1            0.0  18      0.000417   \n",
       "4    2      219       NaN       1            0.0   6      0.000244   \n",
       "\n",
       "   app_channel_confRate  app_device_confRate  hour    ...      V_7  V_15  \\\n",
       "0              0.000168             0.000165     9    ...       11    13   \n",
       "1              0.001661             0.001951     9    ...       29    74   \n",
       "2              0.000338             0.001738     9    ...        7   162   \n",
       "3              0.000350             0.000418     9    ...      139   142   \n",
       "4              0.000309             0.000255     9    ...       44    89   \n",
       "\n",
       "   V_10    V_18   V_19    V_20  V_11  V_12          V_13       V_14  \n",
       "0     9   31514  32252   49232     1     1   1181.209595  29.850218  \n",
       "1    23   16197  13968  239249    12     4   1474.756592   1.926109  \n",
       "2  7047    7930  32683    8575  9025   204    293.065765  89.350601  \n",
       "3   346   15335  26217   49232  1247    42   5029.855469  14.620737  \n",
       "4     4  110713  14359   25976   138     2  19382.593750  27.856543  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T17:53:13.617096Z",
     "start_time": "2018-04-25T17:53:12.122904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train size: ', 124903891)\n",
      "('valid size: ', 20000000)\n",
      "('test size : ', 18790469)\n",
      "('predictors', ['V_14', 'V_15', 'V_12', 'V_13', 'V_10', 'V_11', 'app', 'V_18', 'V_19', 'nextClick', 'app_confRate', 'app_channel_confRate', 'app_device_confRate', 'min', 'channel', 'device', 'V_20', 'day', 'V_4', 'V_7', 'V_2', 'hour', 'V_3', 'sec', 'os'])\n"
     ]
    }
   ],
   "source": [
    "test_df = train_df[len_train:]\n",
    "val_size=20000000\n",
    "val_df = train_df[(len_train-val_size):len_train]\n",
    "train_df = train_df[:(len_train-val_size)]\n",
    "print(\"train size: \", len(train_df))\n",
    "print(\"valid size: \", len(val_df))\n",
    "print(\"test size : \", len(test_df))\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['click_id'] = test_df['click_id'].astype('int')\n",
    "\n",
    "gc.collect()\n",
    "target = 'is_attributed'\n",
    "predictors = list(set(train_df.columns) - set(['is_attributed','click_id'])) \n",
    "categorical = ['app', 'device', 'os', 'channel', 'hour', 'day']\n",
    "print('predictors',predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-04-25T17:10:44.576Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#*****************************************************************************************************************************\n",
    "#*****************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-04-25T17:10:44.589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Training...\n",
      "preparing validation datasets\n",
      "[20]\ttraining's auc: 0.986582\n",
      "[40]\ttraining's auc: 0.989092\n",
      "[60]\ttraining's auc: 0.990876\n",
      "[80]\ttraining's auc: 0.992147\n",
      "[100]\ttraining's auc: 0.993108\n",
      "[120]\ttraining's auc: 0.99384\n",
      "[140]\ttraining's auc: 0.994487\n",
      "[160]\ttraining's auc: 0.994975\n",
      "[180]\ttraining's auc: 0.995404\n",
      "[200]\ttraining's auc: 0.995786\n",
      "**********************************************************\n",
      "{'num_leaves': 317, 'subsample_freq': 1, 'verbose': 0, 'scale_pos_weight': 450, 'learning_rate': 0.2, 'metric': 'auc', 'boosting_type': 'gbdt', 'colsample_bytree': 0.9, 'min_child_samples': 60, 'nthread': 10, 'min_child_weight': 15.0, 'min_split_gain': 0, 'subsample': 0.7, 'max_bin': 255, 'objective': 'binary', 'max_depth': 11}\n",
      "('SCORE ............. : ', 0.98389297404859088)\n",
      "**********************************************************\n",
      "[20]\ttraining's auc: 0.980404\n",
      "[40]\ttraining's auc: 0.982697\n",
      "[60]\ttraining's auc: 0.983703\n",
      "[80]\ttraining's auc: 0.984272\n",
      "[100]\ttraining's auc: 0.984634\n",
      "[120]\ttraining's auc: 0.985022\n",
      "[140]\ttraining's auc: 0.985262\n",
      "[160]\ttraining's auc: 0.985472\n",
      "[180]\ttraining's auc: 0.985626\n",
      "[200]\ttraining's auc: 0.985798\n",
      "**********************************************************\n",
      "{'num_leaves': 377, 'subsample_freq': 1, 'verbose': 0, 'scale_pos_weight': 290, 'learning_rate': 0.2, 'metric': 'auc', 'boosting_type': 'gbdt', 'colsample_bytree': 1.0, 'min_child_samples': 260, 'nthread': 10, 'min_child_weight': 71.0, 'min_split_gain': 0, 'subsample': 0.7, 'max_bin': 1200, 'objective': 'binary', 'max_depth': 4}\n",
      "('SCORE ............. : ', 0.98523114172624993)\n",
      "**********************************************************\n",
      "[20]\ttraining's auc: 0.971839\n",
      "[40]\ttraining's auc: 0.978361\n",
      "[60]\ttraining's auc: 0.981079\n",
      "[80]\ttraining's auc: 0.982286\n",
      "[100]\ttraining's auc: 0.983123\n",
      "[120]\ttraining's auc: 0.983693\n",
      "[140]\ttraining's auc: 0.98404\n",
      "[160]\ttraining's auc: 0.984285\n",
      "[180]\ttraining's auc: 0.984613\n",
      "[200]\ttraining's auc: 0.984807\n",
      "**********************************************************\n",
      "{'num_leaves': 97, 'subsample_freq': 1, 'verbose': 0, 'scale_pos_weight': 370, 'learning_rate': 0.2, 'metric': 'auc', 'boosting_type': 'gbdt', 'colsample_bytree': 0.2, 'min_child_samples': 110, 'nthread': 10, 'min_child_weight': 8.0, 'min_split_gain': 0, 'subsample': 0.7, 'max_bin': 2850, 'objective': 'binary', 'max_depth': 4}\n",
      "('SCORE ............. : ', 0.98447184978537361)\n",
      "**********************************************************\n",
      "[20]\ttraining's auc: 0.982963\n",
      "[40]\ttraining's auc: 0.985283\n",
      "[60]\ttraining's auc: 0.986245\n",
      "[80]\ttraining's auc: 0.986907\n",
      "[100]\ttraining's auc: 0.987434\n",
      "[120]\ttraining's auc: 0.987915\n",
      "[140]\ttraining's auc: 0.988375\n",
      "[160]\ttraining's auc: 0.988788\n",
      "[180]\ttraining's auc: 0.989169\n",
      "[200]\ttraining's auc: 0.989504\n",
      "**********************************************************\n",
      "{'num_leaves': 67, 'subsample_freq': 1, 'verbose': 0, 'scale_pos_weight': 230, 'learning_rate': 0.2, 'metric': 'auc', 'boosting_type': 'gbdt', 'colsample_bytree': 0.7000000000000001, 'min_child_samples': 310, 'nthread': 10, 'min_child_weight': 46.0, 'min_split_gain': 0, 'subsample': 0.7, 'max_bin': 500, 'objective': 'binary', 'max_depth': 8}\n",
      "('SCORE ............. : ', 0.98518090035120098)\n",
      "**********************************************************\n",
      "[20]\ttraining's auc: 0.978169\n",
      "[40]\ttraining's auc: 0.981539\n",
      "[60]\ttraining's auc: 0.982441\n",
      "[80]\ttraining's auc: 0.983111\n",
      "[100]\ttraining's auc: 0.983534\n",
      "[120]\ttraining's auc: 0.983861\n",
      "[140]\ttraining's auc: 0.984107\n",
      "[160]\ttraining's auc: 0.984261\n",
      "[180]\ttraining's auc: 0.984456\n",
      "[200]\ttraining's auc: 0.984584\n",
      "**********************************************************\n",
      "{'num_leaves': 477, 'subsample_freq': 1, 'verbose': 0, 'scale_pos_weight': 370, 'learning_rate': 0.2, 'metric': 'auc', 'boosting_type': 'gbdt', 'colsample_bytree': 0.7000000000000001, 'min_child_samples': 160, 'nthread': 10, 'min_child_weight': 11.0, 'min_split_gain': 0, 'subsample': 0.7, 'max_bin': 250, 'objective': 'binary', 'max_depth': 3}\n",
      "('SCORE ............. : ', 0.98469852309883654)\n",
      "**********************************************************\n",
      "[20]\ttraining's auc: 0.978617\n",
      "[40]\ttraining's auc: 0.981312\n",
      "[60]\ttraining's auc: 0.982262\n",
      "[80]\ttraining's auc: 0.982982\n",
      "[100]\ttraining's auc: 0.983353\n",
      "[120]\ttraining's auc: 0.983748\n",
      "[140]\ttraining's auc: 0.983998\n",
      "[160]\ttraining's auc: 0.984215\n",
      "[180]\ttraining's auc: 0.984381\n",
      "[200]\ttraining's auc: 0.984544\n",
      "**********************************************************\n",
      "{'num_leaves': 207, 'subsample_freq': 1, 'verbose': 0, 'scale_pos_weight': 390, 'learning_rate': 0.2, 'metric': 'auc', 'boosting_type': 'gbdt', 'colsample_bytree': 0.5, 'min_child_samples': 410, 'nthread': 10, 'min_child_weight': 30.0, 'min_split_gain': 0, 'subsample': 0.7, 'max_bin': 3750, 'objective': 'binary', 'max_depth': 3}\n",
      "('SCORE ............. : ', 0.98461763286145842)\n",
      "**********************************************************\n",
      "[20]\ttraining's auc: 0.983127\n",
      "[40]\ttraining's auc: 0.985411\n",
      "[60]\ttraining's auc: 0.986541\n",
      "[80]\ttraining's auc: 0.987334\n",
      "[100]\ttraining's auc: 0.987948\n",
      "[120]\ttraining's auc: 0.988424\n",
      "[140]\ttraining's auc: 0.988949\n",
      "[160]\ttraining's auc: 0.989412\n",
      "[180]\ttraining's auc: 0.989845\n",
      "[200]\ttraining's auc: 0.990243\n",
      "**********************************************************\n",
      "{'num_leaves': 267, 'subsample_freq': 1, 'verbose': 0, 'scale_pos_weight': 330, 'learning_rate': 0.2, 'metric': 'auc', 'boosting_type': 'gbdt', 'colsample_bytree': 0.7000000000000001, 'min_child_samples': 410, 'nthread': 10, 'min_child_weight': 56.0, 'min_split_gain': 0, 'subsample': 0.7, 'max_bin': 2900, 'objective': 'binary', 'max_depth': 7}\n",
      "('SCORE ............. : ', 0.98514492588421732)\n",
      "**********************************************************\n",
      "[20]\ttraining's auc: 0.981492\n",
      "[40]\ttraining's auc: 0.983588\n",
      "[60]\ttraining's auc: 0.984611\n",
      "[80]\ttraining's auc: 0.985166\n",
      "[100]\ttraining's auc: 0.985547\n",
      "[120]\ttraining's auc: 0.985875\n",
      "[140]\ttraining's auc: 0.986151\n",
      "[160]\ttraining's auc: 0.986355\n",
      "[180]\ttraining's auc: 0.986565\n",
      "[200]\ttraining's auc: 0.98676\n",
      "**********************************************************\n",
      "{'num_leaves': 27, 'subsample_freq': 1, 'verbose': 0, 'scale_pos_weight': 240, 'learning_rate': 0.2, 'metric': 'auc', 'boosting_type': 'gbdt', 'colsample_bytree': 0.9, 'min_child_samples': 110, 'nthread': 10, 'min_child_weight': 98.0, 'min_split_gain': 0, 'subsample': 0.7, 'max_bin': 2650, 'objective': 'binary', 'max_depth': 5}\n",
      "('SCORE ............. : ', 0.98541507042139498)\n",
      "**********************************************************\n",
      "[20]\ttraining's auc: 0.975126\n",
      "[40]\ttraining's auc: 0.980399\n",
      "[60]\ttraining's auc: 0.982933\n",
      "[80]\ttraining's auc: 0.984169\n",
      "[100]\ttraining's auc: 0.984905\n",
      "[120]\ttraining's auc: 0.985452\n",
      "[140]\ttraining's auc: 0.985755\n",
      "[160]\ttraining's auc: 0.98604\n",
      "[180]\ttraining's auc: 0.986362\n",
      "[200]\ttraining's auc: 0.986593\n",
      "**********************************************************\n",
      "{'num_leaves': 37, 'subsample_freq': 1, 'verbose': 0, 'scale_pos_weight': 430, 'learning_rate': 0.2, 'metric': 'auc', 'boosting_type': 'gbdt', 'colsample_bytree': 0.2, 'min_child_samples': 160, 'nthread': 10, 'min_child_weight': 80.0, 'min_split_gain': 0, 'subsample': 0.7, 'max_bin': 2500, 'objective': 'binary', 'max_depth': 7}\n",
      "('SCORE ............. : ', 0.98525605897094515)\n",
      "**********************************************************\n",
      "[20]\ttraining's auc: 0.986057\n",
      "[40]\ttraining's auc: 0.988535\n",
      "[60]\ttraining's auc: 0.990198\n",
      "[80]\ttraining's auc: 0.991415\n",
      "[100]\ttraining's auc: 0.992476\n",
      "[120]\ttraining's auc: 0.993289\n",
      "[140]\ttraining's auc: 0.994032\n",
      "[160]\ttraining's auc: 0.994635\n",
      "[180]\ttraining's auc: 0.995113\n",
      "[200]\ttraining's auc: 0.995537\n",
      "**********************************************************\n",
      "{'num_leaves': 397, 'subsample_freq': 1, 'verbose': 0, 'scale_pos_weight': 470, 'learning_rate': 0.2, 'metric': 'auc', 'boosting_type': 'gbdt', 'colsample_bytree': 0.5, 'min_child_samples': 210, 'nthread': 10, 'min_child_weight': 98.0, 'min_split_gain': 0, 'subsample': 0.7, 'max_bin': 1800, 'objective': 'binary', 'max_depth': 10}\n",
      "('SCORE ............. : ', 0.98390570575493375)\n",
      "**********************************************************\n",
      "[20]\ttraining's auc: 0.980161\n",
      "[40]\ttraining's auc: 0.982729\n",
      "[60]\ttraining's auc: 0.98377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80]\ttraining's auc: 0.984329\n",
      "[100]\ttraining's auc: 0.984655\n",
      "[120]\ttraining's auc: 0.984936\n",
      "[140]\ttraining's auc: 0.985202\n",
      "[160]\ttraining's auc: 0.985393\n",
      "[180]\ttraining's auc: 0.985617\n",
      "[200]\ttraining's auc: 0.985768\n",
      "**********************************************************\n",
      "{'num_leaves': 137, 'subsample_freq': 1, 'verbose': 0, 'scale_pos_weight': 410, 'learning_rate': 0.2, 'metric': 'auc', 'boosting_type': 'gbdt', 'colsample_bytree': 0.7000000000000001, 'min_child_samples': 110, 'nthread': 10, 'min_child_weight': 13.0, 'min_split_gain': 0, 'subsample': 0.7, 'max_bin': 3150, 'objective': 'binary', 'max_depth': 4}\n",
      "('SCORE ............. : ', 0.9851875526898064)\n",
      "**********************************************************\n",
      "[20]\ttraining's auc: 0.985307\n",
      "[40]\ttraining's auc: 0.98765\n",
      "[60]\ttraining's auc: 0.989091\n",
      "[80]\ttraining's auc: 0.990396\n",
      "[100]\ttraining's auc: 0.991295\n",
      "[120]\ttraining's auc: 0.992096\n",
      "[140]\ttraining's auc: 0.992806\n",
      "[160]\ttraining's auc: 0.993409\n",
      "[180]\ttraining's auc: 0.993963\n",
      "[200]\ttraining's auc: 0.994437\n",
      "**********************************************************\n",
      "{'num_leaves': 347, 'subsample_freq': 1, 'verbose': 0, 'scale_pos_weight': 430, 'learning_rate': 0.2, 'metric': 'auc', 'boosting_type': 'gbdt', 'colsample_bytree': 0.6000000000000001, 'min_child_samples': 360, 'nthread': 10, 'min_child_weight': 6.0, 'min_split_gain': 0, 'subsample': 0.7, 'max_bin': 3250, 'objective': 'binary', 'max_depth': 9}\n",
      "('SCORE ............. : ', 0.98398373822988205)\n",
      "**********************************************************\n",
      "[20]\ttraining's auc: 0.979226\n",
      "[40]\ttraining's auc: 0.98241\n",
      "[60]\ttraining's auc: 0.983519\n",
      "[80]\ttraining's auc: 0.984125\n",
      "[100]\ttraining's auc: 0.98448\n",
      "[120]\ttraining's auc: 0.984804\n",
      "[140]\ttraining's auc: 0.985056\n",
      "[160]\ttraining's auc: 0.985231\n",
      "[180]\ttraining's auc: 0.985415\n",
      "[200]\ttraining's auc: 0.985572\n",
      "**********************************************************\n",
      "{'num_leaves': 317, 'subsample_freq': 1, 'verbose': 0, 'scale_pos_weight': 100, 'learning_rate': 0.2, 'metric': 'auc', 'boosting_type': 'gbdt', 'colsample_bytree': 0.9, 'min_child_samples': 410, 'nthread': 10, 'min_child_weight': 43.0, 'min_split_gain': 0, 'subsample': 0.7, 'max_bin': 4900, 'objective': 'binary', 'max_depth': 4}\n",
      "('SCORE ............. : ', 0.98526096340201341)\n",
      "**********************************************************\n",
      "[20]\ttraining's auc: 0.975903\n",
      "[40]\ttraining's auc: 0.98368\n",
      "[60]\ttraining's auc: 0.985694\n",
      "[80]\ttraining's auc: 0.986762\n",
      "[100]\ttraining's auc: 0.987477\n",
      "[120]\ttraining's auc: 0.988106\n",
      "[140]\ttraining's auc: 0.988602\n",
      "[160]\ttraining's auc: 0.989103\n",
      "[180]\ttraining's auc: 0.989534\n",
      "[200]\ttraining's auc: 0.989941\n",
      "**********************************************************\n",
      "{'num_leaves': 107, 'subsample_freq': 1, 'verbose': 0, 'scale_pos_weight': 100, 'learning_rate': 0.2, 'metric': 'auc', 'boosting_type': 'gbdt', 'colsample_bytree': 0.30000000000000004, 'min_child_samples': 460, 'nthread': 10, 'min_child_weight': 45.0, 'min_split_gain': 0, 'subsample': 0.7, 'max_bin': 3650, 'objective': 'binary', 'max_depth': 10}\n",
      "('SCORE ............. : ', 0.98493017420895579)\n",
      "**********************************************************\n",
      "[20]\ttraining's auc: 0.983221\n",
      "[40]\ttraining's auc: 0.985388\n",
      "[60]\ttraining's auc: 0.986449\n",
      "[80]\ttraining's auc: 0.98719\n",
      "[100]\ttraining's auc: 0.987801\n",
      "[120]\ttraining's auc: 0.988294\n",
      "[140]\ttraining's auc: 0.988764\n",
      "[160]\ttraining's auc: 0.989158\n",
      "[180]\ttraining's auc: 0.989557\n",
      "[200]\ttraining's auc: 0.989946\n",
      "**********************************************************\n",
      "{'num_leaves': 237, 'subsample_freq': 1, 'verbose': 0, 'scale_pos_weight': 410, 'learning_rate': 0.2, 'metric': 'auc', 'boosting_type': 'gbdt', 'colsample_bytree': 0.5, 'min_child_samples': 260, 'nthread': 10, 'min_child_weight': 72.0, 'min_split_gain': 0, 'subsample': 0.7, 'max_bin': 1150, 'objective': 'binary', 'max_depth': 7}\n",
      "('SCORE ............. : ', 0.98542481613992006)\n",
      "**********************************************************\n",
      "[20]\ttraining's auc: 0.980921\n",
      "[40]\ttraining's auc: 0.983171\n",
      "[60]\ttraining's auc: 0.984308\n",
      "[80]\ttraining's auc: 0.984931\n",
      "[100]\ttraining's auc: 0.985295\n",
      "[120]\ttraining's auc: 0.985605\n",
      "[140]\ttraining's auc: 0.985862\n",
      "[160]\ttraining's auc: 0.986113\n",
      "[180]\ttraining's auc: 0.986321\n",
      "[200]\ttraining's auc: 0.986509\n",
      "**********************************************************\n",
      "{'num_leaves': 67, 'subsample_freq': 1, 'verbose': 0, 'scale_pos_weight': 290, 'learning_rate': 0.2, 'metric': 'auc', 'boosting_type': 'gbdt', 'colsample_bytree': 0.4, 'min_child_samples': 360, 'nthread': 10, 'min_child_weight': 45.0, 'min_split_gain': 0, 'subsample': 0.7, 'max_bin': 4050, 'objective': 'binary', 'max_depth': 5}\n",
      "('SCORE ............. : ', 0.98526105829403787)\n",
      "**********************************************************\n",
      "[20]\ttraining's auc: 0.982762\n",
      "[40]\ttraining's auc: 0.984833\n",
      "[60]\ttraining's auc: 0.98606\n",
      "[80]\ttraining's auc: 0.986877\n",
      "[100]\ttraining's auc: 0.987479\n",
      "[120]\ttraining's auc: 0.988026\n",
      "[140]\ttraining's auc: 0.988481\n",
      "[160]\ttraining's auc: 0.988938\n",
      "[180]\ttraining's auc: 0.989313\n",
      "[200]\ttraining's auc: 0.989671\n",
      "**********************************************************\n",
      "{'num_leaves': 267, 'subsample_freq': 1, 'verbose': 0, 'scale_pos_weight': 310, 'learning_rate': 0.2, 'metric': 'auc', 'boosting_type': 'gbdt', 'colsample_bytree': 0.4, 'min_child_samples': 410, 'nthread': 10, 'min_child_weight': 65.0, 'min_split_gain': 0, 'subsample': 0.7, 'max_bin': 1650, 'objective': 'binary', 'max_depth': 7}\n",
      "('SCORE ............. : ', 0.98497185788330421)\n",
      "**********************************************************\n",
      "[20]\ttraining's auc: 0.980461\n",
      "[40]\ttraining's auc: 0.982979\n",
      "[60]\ttraining's auc: 0.98423\n",
      "[80]\ttraining's auc: 0.984796\n",
      "[100]\ttraining's auc: 0.985186\n",
      "[120]\ttraining's auc: 0.985527\n",
      "[140]\ttraining's auc: 0.985759\n",
      "[160]\ttraining's auc: 0.986027\n",
      "[180]\ttraining's auc: 0.986227\n",
      "[200]\ttraining's auc: 0.986419\n",
      "**********************************************************\n",
      "{'num_leaves': 97, 'subsample_freq': 1, 'verbose': 0, 'scale_pos_weight': 170, 'learning_rate': 0.2, 'metric': 'auc', 'boosting_type': 'gbdt', 'colsample_bytree': 0.4, 'min_child_samples': 260, 'nthread': 10, 'min_child_weight': 45.0, 'min_split_gain': 0, 'subsample': 0.7, 'max_bin': 4850, 'objective': 'binary', 'max_depth': 5}\n",
      "('SCORE ............. : ', 0.98519465890063818)\n",
      "**********************************************************\n",
      "[20]\ttraining's auc: 0.981781\n",
      "[40]\ttraining's auc: 0.98448\n",
      "[60]\ttraining's auc: 0.98547\n",
      "[80]\ttraining's auc: 0.985984\n",
      "[100]\ttraining's auc: 0.986374\n",
      "[120]\ttraining's auc: 0.986757\n",
      "[140]\ttraining's auc: 0.987166\n",
      "[160]\ttraining's auc: 0.987458\n",
      "[180]\ttraining's auc: 0.987764\n",
      "[200]\ttraining's auc: 0.988034\n",
      "**********************************************************\n",
      "{'num_leaves': 317, 'subsample_freq': 1, 'verbose': 0, 'scale_pos_weight': 270, 'learning_rate': 0.2, 'metric': 'auc', 'boosting_type': 'gbdt', 'colsample_bytree': 0.5, 'min_child_samples': 10, 'nthread': 10, 'min_child_weight': 7.0, 'min_split_gain': 0, 'subsample': 0.7, 'max_bin': 4250, 'objective': 'binary', 'max_depth': 6}\n",
      "('SCORE ............. : ', 0.98518690784959939)\n",
      "**********************************************************\n",
      "[20]\ttraining's auc: 0.978306\n",
      "[40]\ttraining's auc: 0.981709\n",
      "[60]\ttraining's auc: 0.982628\n",
      "[80]\ttraining's auc: 0.983249\n",
      "[100]\ttraining's auc: 0.98366\n",
      "[120]\ttraining's auc: 0.983915\n",
      "[140]\ttraining's auc: 0.984132\n",
      "[160]\ttraining's auc: 0.984304\n"
     ]
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "params = {\n",
    "    'learning_rate': 0.2,\n",
    "    #'is_unbalance': 'true', # replaced with scale_pos_weight argument\n",
    "    'num_leaves': 12,  # 2^max_depth - 1\n",
    "    'max_depth': 5,  # -1 means no limit\n",
    "    'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "    'max_bin': 100,  # Number of bucketed bin for feature values\n",
    "    'subsample': 0.95,  # Subsample ratio of the training instance.\n",
    "    'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "    'colsample_bytree': 0.7,  # Subsample ratio of columns when constructing each tree.\n",
    "    'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "    'scale_pos_weight':130 # because training data is extremely unbalanced \n",
    "}\n",
    "print(\"Training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "(bst,best_iteration) = lgb_modelfit_nocv(params, \n",
    "                        train_df, \n",
    "                        val_df, \n",
    "                        predictors, \n",
    "                        target, \n",
    "                        objective='binary', \n",
    "                        metrics='auc',\n",
    "                        early_stopping_rounds=50, \n",
    "                        verbose_eval=True, \n",
    "                        num_boost_round=200, \n",
    "                        categorical_features=categorical)\n",
    "\n",
    "print('[{}]: model training time'.format(time.time() - start_time))\n",
    "del train_df\n",
    "del val_df\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{'num_leaves': 27, 'subsample_freq': 1, 'verbose': 0, 'scale_pos_weight': 240, 'learning_rate': 0.2, 'metric': 'auc', 'boosting_type': 'gbdt', 'colsample_bytree': 0.9, 'min_child_samples': 110, 'nthread': 10, 'min_child_weight': 98.0, 'min_split_gain': 0, 'subsample': 0.7, 'max_bin': 2650, 'objective': 'binary', 'max_depth': 5}\n",
    "('SCORE ............. : ', 0.98541507042139498)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{'num_leaves': 237, 'subsample_freq': 1, 'verbose': 0, 'scale_pos_weight': 410, 'learning_rate': 0.2, 'metric': 'auc', 'boosting_type': 'gbdt', 'colsample_bytree': 0.5, 'min_child_samples': 260, 'nthread': 10, 'min_child_weight': 72.0, 'min_split_gain': 0, 'subsample': 0.7, 'max_bin': 1150, 'objective': 'binary', 'max_depth': 7}\n",
    "('SCORE ............. : ', 0.98542481613992006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric':'auc',\n",
    "        'learning_rate': 0.12,\n",
    "        'num_leaves': 44,\n",
    "        'max_depth': 5,\n",
    "        'min_child_samples': 100, # 100, \n",
    "        'max_bin': 100,\n",
    "        #\"drop_rate\": 0.2,\n",
    "        'subsample': 0.85,\n",
    "        'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.5,\n",
    "        'min_child_weight': 0.0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "        'nthread': 8,\n",
    "        'verbose': 0,\n",
    "        'scale_pos_weight': 130.\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bst = lgb.train(params, \n",
    "                     xgtrain, \n",
    "                     valid_sets= [xgtrain,xgvalid],  \n",
    "                     num_boost_round=2000,\n",
    "                     early_stopping_rounds=150,\n",
    "                     verbose_eval=20)\n",
    "gc.collect()\n",
    "#bst.save_model('lgb_model_train_8_val_9.txt')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bst.save_model('train_8_validation_9.txt',num_iteration=bst.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
